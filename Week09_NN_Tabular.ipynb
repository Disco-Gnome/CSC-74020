{"cells":[{"cell_type":"markdown","id":"8rD50bJhCqhG","metadata":{"id":"8rD50bJhCqhG"},"source":["# CSC 74020 Machine Learning\n","# Week 9: Neural Networks on Tabular Data\n","\n","#### We build a few Neural Networks on tabular data and show how to use the keras model class and layer classes for building Neural Networks (including non-sequential networks)"]},{"cell_type":"code","execution_count":3,"id":"QDPZoT72zbjY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16504,"status":"ok","timestamp":1699492627362,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":300},"id":"QDPZoT72zbjY","outputId":"b5bb08ea-c8f1-40d3-ac0b-c2f0a8512317"},"outputs":[],"source":["# !pip install tensorflow_addons"]},{"cell_type":"code","execution_count":1,"id":"5G0FonjTGJHv","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":16388,"status":"error","timestamp":1712188989669,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":240},"id":"5G0FonjTGJHv","outputId":"349e1d14-d928-4a90-d353-166c4e08d943"},"outputs":[],"source":["from typing import Any, Dict\n","\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","import scipy.special\n","import sklearn.datasets\n","import sklearn.metrics\n","import sklearn.model_selection\n","import sklearn.preprocessing\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.optimizers import Adam\n","# from official.nlp import optimization\n","\n","import tensorflow_addons as tfa\n","\n","from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, ReLU, Add, PReLU"]},{"cell_type":"markdown","id":"_3FQaH0zGJH2","metadata":{"id":"_3FQaH0zGJH2"},"source":["## Build out MLP (standard FF NN) and ResNet Block"]},{"cell_type":"code","execution_count":2,"id":"ysG86H8rGJH7","metadata":{"id":"ysG86H8rGJH7"},"outputs":[],"source":["class MLP(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(MLP, self).__init__(**kwargs )\n","\n","        self.dense1 = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.dense2 = Dense(d_hidden)\n","\n","        # self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","\n","    def call(self, inputs):\n","        x = self.dense1(inputs)\n","        x = self.activation(x)\n","        x = self.dense2(inputs)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        return x"]},{"cell_type":"code","execution_count":4,"id":"zaeGX7zaGJIE","metadata":{"id":"zaeGX7zaGJIE"},"outputs":[],"source":["class ResNetBlock(tf.keras.layers.Layer):\n","    \"\"\"The main building block of `ResNet`.\"\"\"\n","\n","    def __init__( self, d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNetBlock, self).__init__( **kwargs)\n","\n","        self.normalization = BatchNormalization()\n","        self.linear_first = Dense(d_hidden)#d_main, d_hidden, bias_first)\n","        self.activation = ReLU()\n","        self.dropout_first = Dropout(.2)\n","        self.linear_second = Dense(d_main)\n","        self.dropout_second = Dropout(0)\n","        self.skip_connection = True\n","\n","    def call(self, x):\n","        x_input = x\n","        x = self.normalization(x)\n","        x = self.linear_first(x)\n","        x = self.activation(x)\n","        x = self.dropout_first(x)\n","        x = self.linear_second(x)\n","        x = self.dropout_second(x)\n","        if self.skip_connection:\n","            x = x_input + x\n","        return x"]},{"cell_type":"code","execution_count":5,"id":"JpBcRYMMGJIG","metadata":{"id":"JpBcRYMMGJIG"},"outputs":[],"source":["class ResNet(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNet, self).__init__(**kwargs )\n","\n","        self.linear_first = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.resnetblock1 = ResNetBlock(d_main,d_hidden)\n","        self.resnetblock2 = ResNetBlock(d_main,d_hidden)\n","        self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","        # self.output_skip = Dense(1)\n","        # self.add_layer = Add()\n","\n","    def call(self, inputs):\n","        x = self.linear_first(inputs)\n","        #x1 = self.output_skip(inputs)\n","        x = self.resnetblock1(x)\n","        x = self.resnetblock2(x)\n","        x = self.normalization(x)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        #x = self.add_layer([x,x1])\n","        return x"]},{"cell_type":"code","execution_count":6,"id":"1_bGPtdhGJIK","metadata":{"id":"1_bGPtdhGJIK"},"outputs":[],"source":["class ResNetDR(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNetDR, self).__init__(**kwargs )\n","\n","        self.dense1 = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.dense2 = Dense(d_hidden)\n","\n","        # self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","        self.output_skip = Dense(1)\n","        self.add_layer = Add()\n","\n","    def call(self, inputs):\n","        x = self.dense1(inputs)\n","        x1 = self.output_skip(inputs)\n","        x = self.activation(x)\n","        x = self.dense2(inputs)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        x = self.add_layer([x,x1])\n","        return x"]},{"cell_type":"markdown","id":"MxFC2QTPGJIM","metadata":{"id":"MxFC2QTPGJIM"},"source":["### Data"]},{"cell_type":"code","execution_count":7,"id":"VqdtcC36GJIN","metadata":{"id":"VqdtcC36GJIN"},"outputs":[],"source":["# !!! NOTE !!! The dataset splits, preprocessing and other details are\n","# significantly different from those used in the\n","# paper \"Revisiting Deep Learning Models for Tabular Data\",\n","# so the results will be different from the reported in the paper.\n","\n","dataset = sklearn.datasets.fetch_california_housing()\n","task_type = 'regression'\n","\n","# dataset = sklearn.datasets.fetch_covtype()\n","# task_type = 'multiclass'\n","\n","assert task_type in ['binclass', 'multiclass', 'regression']\n","\n","X_all = dataset['data'].astype('float32')\n","y_all = dataset['target'].astype('float32' if task_type == 'regression' else 'int64')\n","if task_type != 'regression':\n","    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n","n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n","\n","X = {}\n","y = {}\n","X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n","    X_all, y_all, train_size=0.8\n",")\n","X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n","    X['train'], y['train'], train_size=0.8\n",")\n","\n","X_orig=X.copy()"]},{"cell_type":"code","execution_count":8,"id":"Yu5Zw2d8GJIP","metadata":{"id":"Yu5Zw2d8GJIP"},"outputs":[],"source":["# not the best way to preprocess features, but enough for the demonstration\n","# preprocess = sklearn.preprocessing.StandardScaler().fit(X_orig['train'])\n","preprocess = sklearn.preprocessing.QuantileTransformer().fit(X_orig['train'])\n","# preprocess = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1),clip=True).fit(X_orig['train'])\n","\n","X = {\n","    k: (2*preprocess.transform(v)-1)#preprocess.transform(v)\n","    for k, v in X_orig.items()\n","}\n","y = {k: v for k, v in y.items()}\n","\n","# !!! CRUCIAL for neural networks when solving regression problems !!!\n","if task_type == 'regression':\n","    y_mean = y['train'].mean().item()\n","    y_std = y['train'].std().item()\n","    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n","else:\n","    y_std = y_mean = None\n","\n","# if task_type != 'multiclass':\n","#     y = {k: v.float() for k, v in y.items()}"]},{"cell_type":"markdown","id":"eLx6kBZuGJIR","metadata":{"id":"eLx6kBZuGJIR"},"source":["### Side Notes: Neighborhood Components Analysis\n","Here we investigate which features help us relate the feature space to the target as if we were using kNN. 2 cells down, notice the last 2 columns are showing the highest weighting which indicates an optimal kNN distance to use should put more weight on Latitude and Longitude as opposed to other features in the models."]},{"cell_type":"code","execution_count":9,"id":"TlGRXHCRGJIU","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":325231,"status":"error","timestamp":1698869829477,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":240},"id":"TlGRXHCRGJIU","outputId":"b5a79601-7a14-4166-f8a6-9815e74e8405"},"outputs":[{"data":{"text/plain":["array([[ 3555.92777338,   695.42934337,   319.90542887,  -223.64682757,\n","          238.37764503, -1374.48275465, -3503.45188749, -2863.51884507],\n","       [  695.42934337,   388.55171233,   -24.36365546,   -38.26317762,\n","           67.09014986,  -187.00878955,  -433.2993336 ,  -573.05953163],\n","       [  319.90542887,   -24.36365546,   810.10477386,   -96.01367228,\n","          -58.63373553,  -527.57552327,  1608.20532435,   689.62971105],\n","       [ -223.64682757,   -38.26317762,   -96.01367228,    90.23908814,\n","          -15.62713775,    94.41293912,   -36.7601109 ,   248.8586409 ],\n","       [  238.37764503,    67.09014986,   -58.63373553,   -15.62713775,\n","           58.57184347,   -99.15022294,  -561.05871631,  -293.23693598],\n","       [-1374.48275465,  -187.00878955,  -527.57552327,    94.41293912,\n","          -99.15022294,  1245.15702944,   319.09516799,  1525.22653504],\n","       [-3503.45188749,  -433.2993336 ,  1608.20532435,   -36.7601109 ,\n","         -561.05871631,   319.09516799, 14294.94018523,   919.09908131],\n","       [-2863.51884507,  -573.05953163,   689.62971105,   248.8586409 ,\n","         -293.23693598,  1525.22653504,   919.09908131, 14290.89958891]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["\n","from sklearn.neighbors import NeighborhoodComponentsAnalysis\n","\n","nca = NeighborhoodComponentsAnalysis(random_state=42)\n","nca.fit(X['train'], np.clip(np.round(1.5*y['train']),-2,3))\n","\n","np.matmul(nca.components_,np.transpose(nca.components_))"]},{"cell_type":"code","execution_count":10,"id":"c2Ep-yxGGJIW","metadata":{"id":"c2Ep-yxGGJIW"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 3.556  0.695  0.32  -0.224  0.238 -1.374 -3.503 -2.864]\n"," [ 0.695  0.389 -0.024 -0.038  0.067 -0.187 -0.433 -0.573]\n"," [ 0.32  -0.024  0.81  -0.096 -0.059 -0.528  1.608  0.69 ]\n"," [-0.224 -0.038 -0.096  0.09  -0.016  0.094 -0.037  0.249]\n"," [ 0.238  0.067 -0.059 -0.016  0.059 -0.099 -0.561 -0.293]\n"," [-1.374 -0.187 -0.528  0.094 -0.099  1.245  0.319  1.525]\n"," [-3.503 -0.433  1.608 -0.037 -0.561  0.319 14.295  0.919]\n"," [-2.864 -0.573  0.69   0.249 -0.293  1.525  0.919 14.291]]\n"]}],"source":["x_nca = np.matmul(nca.components_,np.transpose(nca.components_))\n","print(np.array_str(x_nca/1000., precision=3, suppress_small=True))"]},{"cell_type":"code","execution_count":11,"id":"51XDD4uAGJIY","metadata":{"id":"51XDD4uAGJIY"},"outputs":[],"source":["#create categories for common numeric values (not useful here)\n","\n","# min_support=15\n","# for ii in range(8):\n","#     values = pd.DataFrame(X['train'])[ii].value_counts()\n","#     values=np.sort(values[values>=min_support].index)\n","#     if len(values)>0:\n","#         enc = OneHotEncoder(categories=[list(values)],handle_unknown='ignore')\n","#         X['train']=np.concatenate([X['train'],enc.fit_transform(pd.DataFrame(X['train'])[[ii]]).toarray()],axis=1)\n","#         X['test']=np.concatenate([X['test'],enc.fit_transform(pd.DataFrame(X['test'])[[ii]]).toarray()],axis=1)\n","#         X['val']=np.concatenate([X['val'],enc.fit_transform(pd.DataFrame(X['val'])[[ii]]).toarray()],axis=1)"]},{"cell_type":"code","execution_count":12,"id":"86IMfz08GJIZ","metadata":{"id":"86IMfz08GJIZ"},"outputs":[{"data":{"text/plain":["((13209, 8), (4128, 8), (3303, 8))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X['train'].shape, X['test'].shape, X['val'].shape"]},{"cell_type":"code","execution_count":13,"id":"bn6iJB_sGJIa","metadata":{"id":"bn6iJB_sGJIa"},"outputs":[{"data":{"text/plain":["1.0103194"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#we standardize the target based on the training portion\n","np.sqrt(np.mean(y['val']*y['val']))"]},{"cell_type":"markdown","id":"_A-YCdGmGJIc","metadata":{"id":"_A-YCdGmGJIc"},"source":["### Model Training / Experiments"]},{"cell_type":"code","execution_count":14,"id":"9L86qAHEGJIa","metadata":{"id":"9L86qAHEGJIa"},"outputs":[],"source":["epochs = 25\n","batch_size=128\n","init_lr = 0.001\n","\n","#we dont need these, but can be useful with certain learning rate schedulers\n","# steps_per_epoch = int(len(X['train'])/batch_size)\n","# num_train_steps = steps_per_epoch * epochs\n","# num_warmup_steps = 0"]},{"cell_type":"code","execution_count":15,"id":"nT8ZAFqeGJId","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30805,"status":"ok","timestamp":1699492678368,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":300},"id":"nT8ZAFqeGJId","outputId":"7fdfd774-5a6c-4905-f129-bc02c95c6edc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss.\n","104/104 [==============================] - ETA: 0s - loss: 0.3683 - mse: 0.3683\n","Epoch 00001: val_loss improved from inf to 0.29426, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3683 - mse: 0.3683 - val_loss: 0.2943 - val_mse: 0.2943\n","Epoch 2/25\n"," 88/104 [========================>.....] - ETA: 0s - loss: 0.2881 - mse: 0.2881\n","Epoch 00002: val_loss improved from 0.29426 to 0.28008, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2870 - mse: 0.2870 - val_loss: 0.2801 - val_mse: 0.2801\n","Epoch 3/25\n","103/104 [============================>.] - ETA: 0s - loss: 0.2711 - mse: 0.2711\n","Epoch 00003: val_loss improved from 0.28008 to 0.26538, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 0s 5ms/step - loss: 0.2710 - mse: 0.2710 - val_loss: 0.2654 - val_mse: 0.2654\n","Epoch 4/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2589 - mse: 0.2589\n","Epoch 00004: val_loss did not improve from 0.26538\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2579 - mse: 0.2579 - val_loss: 0.2777 - val_mse: 0.2777\n","Epoch 5/25\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2542\n","Epoch 00005: val_loss improved from 0.26538 to 0.24427, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.2443 - val_mse: 0.2443\n","Epoch 6/25\n","103/104 [============================>.] - ETA: 0s - loss: 0.2405 - mse: 0.2405\n","Epoch 00006: val_loss did not improve from 0.24427\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2406 - mse: 0.2406 - val_loss: 0.2470 - val_mse: 0.2470\n","Epoch 7/25\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.2382 - mse: 0.2382\n","Epoch 00007: val_loss improved from 0.24427 to 0.24039, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2356 - mse: 0.2356 - val_loss: 0.2404 - val_mse: 0.2404\n","Epoch 8/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2323 - mse: 0.2323\n","Epoch 00008: val_loss did not improve from 0.24039\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2325 - mse: 0.2325 - val_loss: 0.2404 - val_mse: 0.2404\n","Epoch 9/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2303 - mse: 0.2303\n","Epoch 00009: val_loss improved from 0.24039 to 0.23250, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2303 - mse: 0.2303 - val_loss: 0.2325 - val_mse: 0.2325\n","Epoch 10/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2264 - mse: 0.2264\n","Epoch 00010: val_loss improved from 0.23250 to 0.23111, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2270 - mse: 0.2270 - val_loss: 0.2311 - val_mse: 0.2311\n","Epoch 11/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2212 - mse: 0.2212\n","Epoch 00011: val_loss improved from 0.23111 to 0.22808, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2225 - mse: 0.2225 - val_loss: 0.2281 - val_mse: 0.2281\n","Epoch 12/25\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2203 - mse: 0.2203\n","Epoch 00012: val_loss improved from 0.22808 to 0.22134, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2205 - mse: 0.2205 - val_loss: 0.2213 - val_mse: 0.2213\n","Epoch 13/25\n","103/104 [============================>.] - ETA: 0s - loss: 0.2186 - mse: 0.2186\n","Epoch 00013: val_loss did not improve from 0.22134\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2186 - mse: 0.2186 - val_loss: 0.2265 - val_mse: 0.2265\n","Epoch 14/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2208 - mse: 0.2208\n","Epoch 00014: val_loss did not improve from 0.22134\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2182 - mse: 0.2182 - val_loss: 0.2285 - val_mse: 0.2285\n","Epoch 15/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2164 - mse: 0.2164\n","Epoch 00015: val_loss did not improve from 0.22134\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2165 - mse: 0.2165 - val_loss: 0.2371 - val_mse: 0.2371\n","Epoch 16/25\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2109 - mse: 0.2109\n","Epoch 00016: val_loss improved from 0.22134 to 0.21708, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2115 - mse: 0.2115 - val_loss: 0.2171 - val_mse: 0.2171\n","Epoch 17/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2139 - mse: 0.2139\n","Epoch 00017: val_loss did not improve from 0.21708\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2133 - mse: 0.2133 - val_loss: 0.2408 - val_mse: 0.2408\n","Epoch 18/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2110 - mse: 0.2110\n","Epoch 00018: val_loss did not improve from 0.21708\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2093 - mse: 0.2093 - val_loss: 0.2201 - val_mse: 0.2201\n","Epoch 19/25\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2107 - mse: 0.2107\n","Epoch 00019: val_loss did not improve from 0.21708\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.2262 - val_mse: 0.2262\n","Epoch 20/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2052 - mse: 0.2052\n","Epoch 00020: val_loss did not improve from 0.21708\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 0.2239 - val_mse: 0.2239\n","Epoch 21/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2077 - mse: 0.2077\n","Epoch 00021: val_loss improved from 0.21708 to 0.21625, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2070 - mse: 0.2070 - val_loss: 0.2162 - val_mse: 0.2162\n","Epoch 22/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2072 - mse: 0.2072\n","Epoch 00022: val_loss improved from 0.21625 to 0.21592, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2074 - mse: 0.2074 - val_loss: 0.2159 - val_mse: 0.2159\n","Epoch 23/25\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2026 - mse: 0.2026\n","Epoch 00023: val_loss did not improve from 0.21592\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2029 - mse: 0.2029 - val_loss: 0.2164 - val_mse: 0.2164\n","Epoch 24/25\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2055 - mse: 0.2055\n","Epoch 00024: val_loss improved from 0.21592 to 0.21485, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2056 - mse: 0.2056 - val_loss: 0.2148 - val_mse: 0.2148\n","Epoch 25/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2007 - mse: 0.2007\n","Epoch 00025: val_loss improved from 0.21485 to 0.21435, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1997 - mse: 0.1997 - val_loss: 0.2143 - val_mse: 0.2143\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2182 - mse: 0.2182\n","[0.46715388 0.46715388]\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2182 - mse: 0.2182\n","[0.46715388 0.46715388]\n"]}],"source":["# Setup checkpoint path (to save the best weights / reduce overfitting)\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","this_model=MLP(512,512)\n","this_model.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","this_model.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback])\n","print(np.sqrt(this_model.evaluate(X['val'], y['val'])))\n","this_model.load_weights(checkpoint_path)\n","print(np.sqrt(this_model.evaluate(X['val'], y['val'])))"]},{"cell_type":"code","execution_count":16,"id":"D6St2-Z8GJIf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17598,"status":"ok","timestamp":1699493224030,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":300},"id":"D6St2-Z8GJIf","outputId":"f0718daa-7f80-49b8-ba88-52adbebd5168"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 Learning Rate: 0.00125\n","Epoch 1/35\n","WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss.\n","104/104 [==============================] - ETA: 0s - loss: 0.5799 - mse: 0.5799\n","Epoch 00001: val_loss improved from inf to 0.32741, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5799 - mse: 0.5799 - val_loss: 0.3274 - val_mse: 0.3274\n","Epoch: 1 Learning Rate: 0.0025\n","Epoch 2/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.3075 - mse: 0.3075\n","Epoch 00002: val_loss improved from 0.32741 to 0.29887, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.3061 - mse: 0.3061 - val_loss: 0.2989 - val_mse: 0.2989\n","Epoch: 2 Learning Rate: 0.005\n","Epoch 3/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2897 - mse: 0.2897\n","Epoch 00003: val_loss improved from 0.29887 to 0.28551, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2903 - mse: 0.2903 - val_loss: 0.2855 - val_mse: 0.2855\n","Epoch: 3 Learning Rate: 0.01\n","Epoch 4/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2818 - mse: 0.2818\n","Epoch 00004: val_loss did not improve from 0.28551\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.3054 - val_mse: 0.3054\n","Epoch: 4 Learning Rate: 0.01\n","Epoch 5/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2671 - mse: 0.2671\n","Epoch 00005: val_loss improved from 0.28551 to 0.27728, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2773 - val_mse: 0.2773\n","Epoch: 5 Learning Rate: 0.01\n","Epoch 6/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2572 - mse: 0.2572\n","Epoch 00006: val_loss improved from 0.27728 to 0.25807, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2566 - mse: 0.2566 - val_loss: 0.2581 - val_mse: 0.2581\n","Epoch: 6 Learning Rate: 0.01\n","Epoch 7/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2556 - mse: 0.2556\n","Epoch 00007: val_loss improved from 0.25807 to 0.25248, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2550 - mse: 0.2550 - val_loss: 0.2525 - val_mse: 0.2525\n","Epoch: 7 Learning Rate: 0.005\n","Epoch 8/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2369 - mse: 0.2369\n","Epoch 00008: val_loss improved from 0.25248 to 0.23462, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2346 - val_mse: 0.2346\n","Epoch: 8 Learning Rate: 0.005\n","Epoch 9/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2325 - mse: 0.2325\n","Epoch 00009: val_loss did not improve from 0.23462\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2343 - mse: 0.2343 - val_loss: 0.2431 - val_mse: 0.2431\n","Epoch: 9 Learning Rate: 0.005\n","Epoch 10/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2297 - mse: 0.2297\n","Epoch 00010: val_loss improved from 0.23462 to 0.23338, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2296 - mse: 0.2296 - val_loss: 0.2334 - val_mse: 0.2334\n","Epoch: 10 Learning Rate: 0.005\n","Epoch 11/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2274 - mse: 0.2274\n","Epoch 00011: val_loss improved from 0.23338 to 0.23119, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2273 - mse: 0.2273 - val_loss: 0.2312 - val_mse: 0.2312\n","Epoch: 11 Learning Rate: 0.005\n","Epoch 12/35\n"," 93/104 [=========================>....] - ETA: 0s - loss: 0.2254 - mse: 0.2254\n","Epoch 00012: val_loss did not improve from 0.23119\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2264 - mse: 0.2264 - val_loss: 0.2335 - val_mse: 0.2335\n","Epoch: 12 Learning Rate: 0.005\n","Epoch 13/35\n"," 94/104 [==========================>...] - ETA: 0s - loss: 0.2258 - mse: 0.2258\n","Epoch 00013: val_loss improved from 0.23119 to 0.23118, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2252 - mse: 0.2252 - val_loss: 0.2312 - val_mse: 0.2312\n","Epoch: 13 Learning Rate: 0.005\n","Epoch 14/35\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2240 - mse: 0.2240\n","Epoch 00014: val_loss did not improve from 0.23118\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2240 - mse: 0.2240 - val_loss: 0.2325 - val_mse: 0.2325\n","Epoch: 14 Learning Rate: 0.005\n","Epoch 15/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2190 - mse: 0.2190\n","Epoch 00015: val_loss improved from 0.23118 to 0.22536, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2197 - mse: 0.2197 - val_loss: 0.2254 - val_mse: 0.2254\n","Epoch: 15 Learning Rate: 0.0025\n","Epoch 16/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2102 - mse: 0.2102\n","Epoch 00016: val_loss improved from 0.22536 to 0.21953, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2092 - mse: 0.2092 - val_loss: 0.2195 - val_mse: 0.2195\n","Epoch: 16 Learning Rate: 0.0025\n","Epoch 17/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2089 - mse: 0.2089\n","Epoch 00017: val_loss improved from 0.21953 to 0.21948, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 0s 5ms/step - loss: 0.2071 - mse: 0.2071 - val_loss: 0.2195 - val_mse: 0.2195\n","Epoch: 17 Learning Rate: 0.0025\n","Epoch 18/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2074 - mse: 0.2074\n","Epoch 00018: val_loss did not improve from 0.21948\n","104/104 [==============================] - 0s 5ms/step - loss: 0.2075 - mse: 0.2075 - val_loss: 0.2202 - val_mse: 0.2202\n","Epoch: 18 Learning Rate: 0.0025\n","Epoch 19/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2084 - mse: 0.2084\n","Epoch 00019: val_loss improved from 0.21948 to 0.21631, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2075 - mse: 0.2075 - val_loss: 0.2163 - val_mse: 0.2163\n","Epoch: 19 Learning Rate: 0.0025\n","Epoch 20/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2050 - mse: 0.2050\n","Epoch 00020: val_loss improved from 0.21631 to 0.21532, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.2153 - val_mse: 0.2153\n","Epoch: 20 Learning Rate: 0.0025\n","Epoch 21/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2053 - mse: 0.2053\n","Epoch 00021: val_loss did not improve from 0.21532\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2067 - mse: 0.2067 - val_loss: 0.2188 - val_mse: 0.2188\n","Epoch: 21 Learning Rate: 0.0025\n","Epoch 22/35\n"," 93/104 [=========================>....] - ETA: 0s - loss: 0.2018 - mse: 0.2018\n","Epoch 00022: val_loss improved from 0.21532 to 0.21388, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2026 - mse: 0.2026 - val_loss: 0.2139 - val_mse: 0.2139\n","Epoch: 22 Learning Rate: 0.0025\n","Epoch 23/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2021 - mse: 0.2021\n","Epoch 00023: val_loss did not improve from 0.21388\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2018 - mse: 0.2018 - val_loss: 0.2154 - val_mse: 0.2154\n","Epoch: 23 Learning Rate: 0.00125\n","Epoch 24/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.1956 - mse: 0.1956\n","Epoch 00024: val_loss improved from 0.21388 to 0.21175, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.1962 - mse: 0.1962 - val_loss: 0.2117 - val_mse: 0.2117\n","Epoch: 24 Learning Rate: 0.00125\n","Epoch 25/35\n"," 91/104 [=========================>....] - ETA: 0s - loss: 0.1936 - mse: 0.1936\n","Epoch 00025: val_loss did not improve from 0.21175\n","104/104 [==============================] - 0s 5ms/step - loss: 0.1953 - mse: 0.1953 - val_loss: 0.2134 - val_mse: 0.2134\n","Epoch: 25 Learning Rate: 0.00125\n","Epoch 26/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.1952 - mse: 0.1952\n","Epoch 00026: val_loss improved from 0.21175 to 0.21063, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1940 - mse: 0.1940 - val_loss: 0.2106 - val_mse: 0.2106\n","Epoch: 26 Learning Rate: 0.00125\n","Epoch 27/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.1928 - mse: 0.1928\n","Epoch 00027: val_loss improved from 0.21063 to 0.21057, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1934 - mse: 0.1934 - val_loss: 0.2106 - val_mse: 0.2106\n","Epoch: 27 Learning Rate: 0.00125\n","Epoch 28/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.1947 - mse: 0.1947\n","Epoch 00028: val_loss improved from 0.21057 to 0.20776, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1930 - mse: 0.1930 - val_loss: 0.2078 - val_mse: 0.2078\n","Epoch: 28 Learning Rate: 0.00125\n","Epoch 29/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909\n","Epoch 00029: val_loss did not improve from 0.20776\n","104/104 [==============================] - 0s 5ms/step - loss: 0.1921 - mse: 0.1921 - val_loss: 0.2114 - val_mse: 0.2114\n","Epoch: 29 Learning Rate: 0.00125\n","Epoch 30/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.1919 - mse: 0.1919\n","Epoch 00030: val_loss did not improve from 0.20776\n","104/104 [==============================] - 0s 5ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.2097 - val_mse: 0.2097\n","Epoch: 30 Learning Rate: 0.00125\n","Epoch 31/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.1907 - mse: 0.1907\n","Epoch 00031: val_loss improved from 0.20776 to 0.20755, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.1916 - mse: 0.1916 - val_loss: 0.2075 - val_mse: 0.2075\n","Epoch: 31 Learning Rate: 0.000625\n","Epoch 32/35\n"," 93/104 [=========================>....] - ETA: 0s - loss: 0.1873 - mse: 0.1873\n","Epoch 00032: val_loss improved from 0.20755 to 0.20715, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.1878 - mse: 0.1878 - val_loss: 0.2072 - val_mse: 0.2072\n","Epoch: 32 Learning Rate: 0.000625\n","Epoch 33/35\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.1881 - mse: 0.1881\n","Epoch 00033: val_loss improved from 0.20715 to 0.20655, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1874 - mse: 0.1874 - val_loss: 0.2065 - val_mse: 0.2065\n","Epoch: 33 Learning Rate: 0.000625\n","Epoch 34/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.1853 - mse: 0.1853\n","Epoch 00034: val_loss did not improve from 0.20655\n","104/104 [==============================] - 0s 5ms/step - loss: 0.1869 - mse: 0.1869 - val_loss: 0.2074 - val_mse: 0.2074\n","Epoch: 34 Learning Rate: 0.000625\n","Epoch 35/35\n"," 94/104 [==========================>...] - ETA: 0s - loss: 0.1869 - mse: 0.1869\n","Epoch 00035: val_loss improved from 0.20655 to 0.20581, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 5ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.2058 - val_mse: 0.2058\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2027 - mse: 0.2027\n","[0.45019351 0.45019351]\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2027 - mse: 0.2027\n","[0.45019351 0.45019351]\n"]}],"source":["epochs=35\n","\n","def warmup_and_step_decay(epoch):\n","    initial_lrate = 0.01\n","    drop = 0.5\n","    epochs_drop = 8\n","    warmup=.5\n","    warmup_steps=2\n","    if epoch<=warmup_steps:\n","        lrate = pow(warmup,warmup_steps-epoch+1)*initial_lrate\n","    else:\n","        lrate = initial_lrate * math.pow(drop,  math.floor((1+epoch)/epochs_drop))\n","    print(\"Epoch: \"+str(epoch)+\" Learning Rate: \"+str(lrate))\n","    return lrate\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(warmup_and_step_decay)\n","\n","# Setup checkpoint path\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","\n","ResModelDR = ResNetDR(256,256)\n","ResModelDR.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","ResModelDR.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback,lr_callback])\n","print(np.sqrt(ResModelDR.evaluate(X['val'], y['val'])))\n","ResModelDR.load_weights(checkpoint_path)\n","print(np.sqrt(ResModelDR.evaluate(X['val'], y['val'])))"]},{"cell_type":"code","execution_count":17,"id":"rd1kRD5kGJIg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49033,"status":"ok","timestamp":1698869914687,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":240},"id":"rd1kRD5kGJIg","outputId":"952a2178-1a4c-4b8d-d592-c278c43e948c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 Learning Rate: 0.00125\n","Epoch 1/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.4591 - mse: 0.4591\n","Epoch 00001: val_loss improved from inf to 0.49805, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4591 - mse: 0.4591 - val_loss: 0.4980 - val_mse: 0.4980\n","Epoch: 1 Learning Rate: 0.0025\n","Epoch 2/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.3363 - mse: 0.3363\n","Epoch 00002: val_loss improved from 0.49805 to 0.44167, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3363 - mse: 0.3363 - val_loss: 0.4417 - val_mse: 0.4417\n","Epoch: 2 Learning Rate: 0.005\n","Epoch 3/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.3247 - mse: 0.3247\n","Epoch 00003: val_loss improved from 0.44167 to 0.33805, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.3230 - mse: 0.3230 - val_loss: 0.3380 - val_mse: 0.3380\n","Epoch: 3 Learning Rate: 0.01\n","Epoch 4/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.3163 - mse: 0.3163\n","Epoch 00004: val_loss did not improve from 0.33805\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3167 - mse: 0.3167 - val_loss: 0.3502 - val_mse: 0.3502\n","Epoch: 4 Learning Rate: 0.01\n","Epoch 5/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2933 - mse: 0.2933\n","Epoch 00005: val_loss improved from 0.33805 to 0.32944, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2936 - mse: 0.2936 - val_loss: 0.3294 - val_mse: 0.3294\n","Epoch: 5 Learning Rate: 0.01\n","Epoch 6/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2926 - mse: 0.2926\n","Epoch 00006: val_loss improved from 0.32944 to 0.27286, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2913 - mse: 0.2913 - val_loss: 0.2729 - val_mse: 0.2729\n","Epoch: 6 Learning Rate: 0.01\n","Epoch 7/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2748 - mse: 0.2748\n","Epoch 00007: val_loss did not improve from 0.27286\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2732 - mse: 0.2732 - val_loss: 0.3824 - val_mse: 0.3824\n","Epoch: 7 Learning Rate: 0.005\n","Epoch 8/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2517\n","Epoch 00008: val_loss improved from 0.27286 to 0.24038, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2510 - mse: 0.2510 - val_loss: 0.2404 - val_mse: 0.2404\n","Epoch: 8 Learning Rate: 0.005\n","Epoch 9/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2525 - mse: 0.2525\n","Epoch 00009: val_loss improved from 0.24038 to 0.22303, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2529 - mse: 0.2529 - val_loss: 0.2230 - val_mse: 0.2230\n","Epoch: 9 Learning Rate: 0.005\n","Epoch 10/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2439 - mse: 0.2439\n","Epoch 00010: val_loss did not improve from 0.22303\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2461 - val_mse: 0.2461\n","Epoch: 10 Learning Rate: 0.005\n","Epoch 11/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2502 - mse: 0.2502\n","Epoch 00011: val_loss did not improve from 0.22303\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2504 - mse: 0.2504 - val_loss: 0.2466 - val_mse: 0.2466\n","Epoch: 11 Learning Rate: 0.005\n","Epoch 12/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2453 - mse: 0.2453\n","Epoch 00012: val_loss did not improve from 0.22303\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2460 - mse: 0.2460 - val_loss: 0.2495 - val_mse: 0.2495\n","Epoch: 12 Learning Rate: 0.005\n","Epoch 13/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2419 - mse: 0.2419\n","Epoch 00013: val_loss did not improve from 0.22303\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2603 - val_mse: 0.2603\n","Epoch: 13 Learning Rate: 0.005\n","Epoch 14/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2414 - mse: 0.2414\n","Epoch 00014: val_loss did not improve from 0.22303\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2411 - mse: 0.2411 - val_loss: 0.2289 - val_mse: 0.2289\n","Epoch: 14 Learning Rate: 0.005\n","Epoch 15/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2397 - mse: 0.2397\n","Epoch 00015: val_loss improved from 0.22303 to 0.21916, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2192 - val_mse: 0.2192\n","Epoch: 15 Learning Rate: 0.0025\n","Epoch 16/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2280 - mse: 0.2280\n","Epoch 00016: val_loss improved from 0.21916 to 0.20954, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2277 - mse: 0.2277 - val_loss: 0.2095 - val_mse: 0.2095\n","Epoch: 16 Learning Rate: 0.0025\n","Epoch 17/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2258 - mse: 0.2258\n","Epoch 00017: val_loss improved from 0.20954 to 0.20589, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2253 - mse: 0.2253 - val_loss: 0.2059 - val_mse: 0.2059\n","Epoch: 17 Learning Rate: 0.0025\n","Epoch 18/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2297 - mse: 0.2297\n","Epoch 00018: val_loss did not improve from 0.20589\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2301 - mse: 0.2301 - val_loss: 0.2169 - val_mse: 0.2169\n","Epoch: 18 Learning Rate: 0.0025\n","Epoch 19/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2298 - mse: 0.2298\n","Epoch 00019: val_loss did not improve from 0.20589\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2302 - mse: 0.2302 - val_loss: 0.2092 - val_mse: 0.2092\n","Epoch: 19 Learning Rate: 0.0025\n","Epoch 20/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2276 - mse: 0.2276\n","Epoch 00020: val_loss did not improve from 0.20589\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2279 - mse: 0.2279 - val_loss: 0.2182 - val_mse: 0.2182\n","Epoch: 20 Learning Rate: 0.0025\n","Epoch 21/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2229 - mse: 0.2229\n","Epoch 00021: val_loss did not improve from 0.20589\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2225 - mse: 0.2225 - val_loss: 0.2190 - val_mse: 0.2190\n","Epoch: 21 Learning Rate: 0.0025\n","Epoch 22/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2244 - mse: 0.2244\n","Epoch 00022: val_loss improved from 0.20589 to 0.20539, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2247 - mse: 0.2247 - val_loss: 0.2054 - val_mse: 0.2054\n","Epoch: 22 Learning Rate: 0.0025\n","Epoch 23/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2262 - mse: 0.2262\n","Epoch 00023: val_loss did not improve from 0.20539\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2261 - mse: 0.2261 - val_loss: 0.2077 - val_mse: 0.2077\n","Epoch: 23 Learning Rate: 0.00125\n","Epoch 24/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2109 - mse: 0.2109\n","Epoch 00024: val_loss did not improve from 0.20539\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2126 - mse: 0.2126 - val_loss: 0.2065 - val_mse: 0.2065\n","Epoch: 24 Learning Rate: 0.00125\n","Epoch 25/35\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2135 - mse: 0.2135\n","Epoch 00025: val_loss improved from 0.20539 to 0.19908, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2136 - mse: 0.2136 - val_loss: 0.1991 - val_mse: 0.1991\n","Epoch: 25 Learning Rate: 0.00125\n","Epoch 26/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2077 - mse: 0.2077\n","Epoch 00026: val_loss did not improve from 0.19908\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2084 - mse: 0.2084 - val_loss: 0.2031 - val_mse: 0.2031\n","Epoch: 26 Learning Rate: 0.00125\n","Epoch 27/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2092 - mse: 0.2092\n","Epoch 00027: val_loss did not improve from 0.19908\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2097 - mse: 0.2097 - val_loss: 0.2029 - val_mse: 0.2029\n","Epoch: 27 Learning Rate: 0.00125\n","Epoch 28/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2073 - mse: 0.2073\n","Epoch 00028: val_loss improved from 0.19908 to 0.19609, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.1961 - val_mse: 0.1961\n","Epoch: 28 Learning Rate: 0.00125\n","Epoch 29/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2163 - mse: 0.2163\n","Epoch 00029: val_loss did not improve from 0.19609\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2151 - mse: 0.2151 - val_loss: 0.2027 - val_mse: 0.2027\n","Epoch: 29 Learning Rate: 0.00125\n","Epoch 30/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2052 - mse: 0.2052\n","Epoch 00030: val_loss improved from 0.19609 to 0.19286, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2043 - mse: 0.2043 - val_loss: 0.1929 - val_mse: 0.1929\n","Epoch: 30 Learning Rate: 0.00125\n","Epoch 31/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2064 - mse: 0.2064\n","Epoch 00031: val_loss did not improve from 0.19286\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2064 - mse: 0.2064 - val_loss: 0.1993 - val_mse: 0.1993\n","Epoch: 31 Learning Rate: 0.000625\n","Epoch 32/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2076 - mse: 0.2076\n","Epoch 00032: val_loss improved from 0.19286 to 0.19076, saving model to model_checkpoint\\checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2074 - mse: 0.2074 - val_loss: 0.1908 - val_mse: 0.1908\n","Epoch: 32 Learning Rate: 0.000625\n","Epoch 33/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2002 - mse: 0.2002\n","Epoch 00033: val_loss did not improve from 0.19076\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2002 - mse: 0.2002 - val_loss: 0.1919 - val_mse: 0.1919\n","Epoch: 33 Learning Rate: 0.000625\n","Epoch 34/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.1991 - mse: 0.1991\n","Epoch 00034: val_loss did not improve from 0.19076\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1987 - mse: 0.1987 - val_loss: 0.1969 - val_mse: 0.1969\n","Epoch: 34 Learning Rate: 0.000625\n","Epoch 35/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2014 - mse: 0.2014\n","Epoch 00035: val_loss did not improve from 0.19076\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2014 - mse: 0.2014 - val_loss: 0.1936 - val_mse: 0.1936\n","104/104 [==============================] - 0s 2ms/step - loss: 0.1887 - mse: 0.1887\n","[0.4344149 0.4344149]\n","104/104 [==============================] - 0s 2ms/step - loss: 0.1864 - mse: 0.1864\n","[0.43173493 0.43173493]\n"]}],"source":["epochs=35\n","\n","def warmup_and_step_decay(epoch):\n","    initial_lrate = 0.01\n","    drop = 0.5\n","    epochs_drop = 8\n","    warmup=.5\n","    warmup_steps=2\n","    if epoch<=warmup_steps:\n","        lrate = pow(warmup,warmup_steps-epoch+1)*initial_lrate\n","    else:\n","        lrate = initial_lrate * math.pow(drop,  math.floor((1+epoch)/epochs_drop))\n","    print(\"Epoch: \"+str(epoch)+\" Learning Rate: \"+str(lrate))\n","    return lrate\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(warmup_and_step_decay)\n","\n","# Setup checkpoint path\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","\n","ResModel = ResNet(128,256)\n","ResModel.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","ResModel.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback,lr_callback])\n","print(np.sqrt(ResModel.evaluate(X['val'], y['val'])))\n","ResModel.load_weights(checkpoint_path)\n","print(np.sqrt(ResModel.evaluate(X['val'], y['val'])))"]},{"cell_type":"code","execution_count":18,"id":"mSmlyPjuGJIj","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1698869914687,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"},"user_tz":240},"id":"mSmlyPjuGJIj","outputId":"ea11d0a5-149d-4933-e8aa-95fa6730893c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"res_net\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              multiple                  1152      \n","_________________________________________________________________\n","res_net_block (ResNetBlock)  multiple                  66432     \n","_________________________________________________________________\n","res_net_block_1 (ResNetBlock multiple                  66432     \n","_________________________________________________________________\n","batch_normalization_2 (Batch multiple                  512       \n","_________________________________________________________________\n","p_re_lu_2 (PReLU)            multiple                  128       \n","_________________________________________________________________\n","dense_12 (Dense)             multiple                  129       \n","=================================================================\n","Total params: 134,785\n","Trainable params: 134,017\n","Non-trainable params: 768\n","_________________________________________________________________\n"]}],"source":["ResModel.summary()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"tf18","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}
